// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple aarch64-none-linux-gnu -target-feature +sve \
// RUN: -disable-O0-optnone -mvscale-min=4 -mvscale-max=4 \
// RUN:  -emit-llvm -o - %s | opt -S -passes=sroa | FileCheck %s

// REQUIRES: aarch64-registered-target

#include <arm_sve.h>

#define N 512

typedef svint8_t fixed_int8_t __attribute__((arm_sve_vector_bits(N)));
typedef svint16_t fixed_int16_t __attribute__((arm_sve_vector_bits(N)));
typedef svint32_t fixed_int32_t __attribute__((arm_sve_vector_bits(N)));
typedef svint64_t fixed_int64_t __attribute__((arm_sve_vector_bits(N)));

typedef svuint8_t fixed_uint8_t __attribute__((arm_sve_vector_bits(N)));
typedef svuint16_t fixed_uint16_t __attribute__((arm_sve_vector_bits(N)));
typedef svuint32_t fixed_uint32_t __attribute__((arm_sve_vector_bits(N)));
typedef svuint64_t fixed_uint64_t __attribute__((arm_sve_vector_bits(N)));

typedef svfloat16_t fixed_float16_t __attribute__((arm_sve_vector_bits(N)));
typedef svfloat32_t fixed_float32_t __attribute__((arm_sve_vector_bits(N)));
typedef svfloat64_t fixed_float64_t __attribute__((arm_sve_vector_bits(N)));

typedef svbool_t fixed_bool_t __attribute__((arm_sve_vector_bits(N)));

// CHECK-LABEL: @lshift_i8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL_COERCE:%.*]] = alloca <vscale x 16 x i8>, align 16
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <vscale x 16 x i8> [[A_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP1:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP0]], i64 0)
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <vscale x 16 x i8> [[B_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP3:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP2]], i64 0)
// CHECK-NEXT:    [[EXT:%.*]] = bytecast exact <64 x b8> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    [[EXT3:%.*]] = bytecast exact <64 x b8> [[TMP3]] to <64 x i8>
// CHECK-NEXT:    [[SHL:%.*]] = shl <64 x i8> [[EXT]], [[EXT3]]
// CHECK-NEXT:    [[UNPROMOTION:%.*]] = bitcast <64 x i8> [[SHL]] to <64 x b8>
// CHECK-NEXT:    store <64 x b8> [[UNPROMOTION]], ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <vscale x 16 x i8>, ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP4]]
//
fixed_int8_t lshift_i8(fixed_int8_t a, fixed_int8_t b) {
  return a << b;
}

// CHECK-LABEL: @rshift_i8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL_COERCE:%.*]] = alloca <vscale x 16 x i8>, align 16
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <vscale x 16 x i8> [[A_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP1:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP0]], i64 0)
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <vscale x 16 x i8> [[B_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP3:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP2]], i64 0)
// CHECK-NEXT:    [[EXT:%.*]] = bytecast exact <64 x b8> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    [[EXT3:%.*]] = bytecast exact <64 x b8> [[TMP3]] to <64 x i8>
// CHECK-NEXT:    [[SHR:%.*]] = lshr <64 x i8> [[EXT]], [[EXT3]]
// CHECK-NEXT:    [[UNPROMOTION:%.*]] = bitcast <64 x i8> [[SHR]] to <64 x b8>
// CHECK-NEXT:    store <64 x b8> [[UNPROMOTION]], ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <vscale x 16 x i8>, ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP4]]
//
fixed_int8_t rshift_i8(fixed_int8_t a, fixed_int8_t b) {
  return a >> b;
}

// CHECK-LABEL: @lshift_u8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL_COERCE:%.*]] = alloca <vscale x 16 x i8>, align 16
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <vscale x 16 x i8> [[A_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP1:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP0]], i64 0)
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <vscale x 16 x i8> [[B_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP3:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP2]], i64 0)
// CHECK-NEXT:    [[EXT:%.*]] = bytecast exact <64 x b8> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    [[EXT3:%.*]] = bytecast exact <64 x b8> [[TMP3]] to <64 x i8>
// CHECK-NEXT:    [[SHL:%.*]] = shl <64 x i8> [[EXT]], [[EXT3]]
// CHECK-NEXT:    [[UNPROMOTION:%.*]] = bitcast <64 x i8> [[SHL]] to <64 x b8>
// CHECK-NEXT:    store <64 x b8> [[UNPROMOTION]], ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <vscale x 16 x i8>, ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP4]]
//
fixed_uint8_t lshift_u8(fixed_uint8_t a, fixed_uint8_t b) {
  return a << b;
}

// CHECK-LABEL: @rshift_u8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL_COERCE:%.*]] = alloca <vscale x 16 x i8>, align 16
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <vscale x 16 x i8> [[A_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP1:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP0]], i64 0)
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <vscale x 16 x i8> [[B_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP3:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP2]], i64 0)
// CHECK-NEXT:    [[EXT:%.*]] = bytecast exact <64 x b8> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    [[EXT3:%.*]] = bytecast exact <64 x b8> [[TMP3]] to <64 x i8>
// CHECK-NEXT:    [[SHR:%.*]] = lshr <64 x i8> [[EXT]], [[EXT3]]
// CHECK-NEXT:    [[UNPROMOTION:%.*]] = bitcast <64 x i8> [[SHR]] to <64 x b8>
// CHECK-NEXT:    store <64 x b8> [[UNPROMOTION]], ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <vscale x 16 x i8>, ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP4]]
//
fixed_uint8_t rshift_u8(fixed_uint8_t a, fixed_uint8_t b) {
  return a >> b;
}

// CHECK-LABEL: @lshift_i16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[B:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[B_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SHL:%.*]] = shl <32 x i16> [[A]], [[B]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.v32i16(<vscale x 8 x i16> poison, <32 x i16> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 8 x i16> [[CAST_SCALABLE]]
//
fixed_int16_t lshift_i16(fixed_int16_t a, fixed_int16_t b) {
  return a << b;
}

// CHECK-LABEL: @rshift_i16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[B:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[B_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SHR:%.*]] = ashr <32 x i16> [[A]], [[B]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.v32i16(<vscale x 8 x i16> poison, <32 x i16> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 8 x i16> [[CAST_SCALABLE]]
//
fixed_int16_t rshift_i16(fixed_int16_t a, fixed_int16_t b) {
  return a >> b;
}

// CHECK-LABEL: @lshift_u16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[B:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[B_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SHL:%.*]] = shl <32 x i16> [[A]], [[B]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.v32i16(<vscale x 8 x i16> poison, <32 x i16> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 8 x i16> [[CAST_SCALABLE]]
//
fixed_uint16_t lshift_u16(fixed_uint16_t a, fixed_uint16_t b) {
  return a << b;
}

// CHECK-LABEL: @rshift_u16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[B:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[B_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SHR:%.*]] = lshr <32 x i16> [[A]], [[B]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.v32i16(<vscale x 8 x i16> poison, <32 x i16> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 8 x i16> [[CAST_SCALABLE]]
//
fixed_uint16_t rshift_u16(fixed_uint16_t a, fixed_uint16_t b) {
  return a >> b;
}

// CHECK-LABEL: @lshift_i32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[B:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[B_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SHL:%.*]] = shl <16 x i32> [[A]], [[B]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.v16i32(<vscale x 4 x i32> poison, <16 x i32> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 4 x i32> [[CAST_SCALABLE]]
//
fixed_int32_t lshift_i32(fixed_int32_t a, fixed_int32_t b) {
  return a << b;
}

// CHECK-LABEL: @rshift_i32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[B:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[B_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SHR:%.*]] = ashr <16 x i32> [[A]], [[B]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.v16i32(<vscale x 4 x i32> poison, <16 x i32> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 4 x i32> [[CAST_SCALABLE]]
//
fixed_int32_t rshift_i32(fixed_int32_t a, fixed_int32_t b) {
  return a >> b;
}

// CHECK-LABEL: @lshift_u32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[B:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[B_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SHL:%.*]] = shl <16 x i32> [[A]], [[B]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.v16i32(<vscale x 4 x i32> poison, <16 x i32> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 4 x i32> [[CAST_SCALABLE]]
//
fixed_uint32_t lshift_u32(fixed_uint32_t a, fixed_uint32_t b) {
  return a << b;
}

// CHECK-LABEL: @rshift_u32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[B:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[B_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SHR:%.*]] = lshr <16 x i32> [[A]], [[B]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.v16i32(<vscale x 4 x i32> poison, <16 x i32> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 4 x i32> [[CAST_SCALABLE]]
//
fixed_uint32_t rshift_u32(fixed_uint32_t a, fixed_uint32_t b) {
  return a >> b;
}

// CHECK-LABEL: @lshift_i64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[B:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[B_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SHL:%.*]] = shl <8 x i64> [[A]], [[B]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.v8i64(<vscale x 2 x i64> poison, <8 x i64> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 2 x i64> [[CAST_SCALABLE]]
//
fixed_int64_t lshift_i64(fixed_int64_t a, fixed_int64_t b) {
  return a << b;
}

// CHECK-LABEL: @rshift_i64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[B:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[B_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SHR:%.*]] = ashr <8 x i64> [[A]], [[B]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.v8i64(<vscale x 2 x i64> poison, <8 x i64> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 2 x i64> [[CAST_SCALABLE]]
//
fixed_int64_t rshift_i64(fixed_int64_t a, fixed_int64_t b) {
  return a >> b;
}

// CHECK-LABEL: @lshift_u64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[B:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[B_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SHL:%.*]] = shl <8 x i64> [[A]], [[B]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.v8i64(<vscale x 2 x i64> poison, <8 x i64> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 2 x i64> [[CAST_SCALABLE]]
//
fixed_uint64_t lshift_u64(fixed_uint64_t a, fixed_uint64_t b) {
  return a << b;
}

// CHECK-LABEL: @rshift_u64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[B:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[B_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SHR:%.*]] = lshr <8 x i64> [[A]], [[B]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.v8i64(<vscale x 2 x i64> poison, <8 x i64> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 2 x i64> [[CAST_SCALABLE]]
//
fixed_uint64_t rshift_u64(fixed_uint64_t a, fixed_uint64_t b) {
  return a >> b;
}

// CHECK-LABEL: @lshift_i8_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL_COERCE:%.*]] = alloca <vscale x 16 x i8>, align 16
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <vscale x 16 x i8> [[A_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP1:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP0]], i64 0)
// CHECK-NEXT:    [[EXT:%.*]] = bytecast exact <64 x b8> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    [[CONV:%.*]] = bytecast exact b8 [[B:%.*]] to i8
// CHECK-NEXT:    [[CONV2:%.*]] = sext i8 [[CONV]] to i32
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <64 x i32> poison, i32 [[CONV2]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <64 x i32> [[SPLAT_SPLATINSERT]], <64 x i32> poison, <64 x i32> zeroinitializer
// CHECK-NEXT:    [[EXT3:%.*]] = trunc <64 x i32> [[SPLAT_SPLAT]] to <64 x i8>
// CHECK-NEXT:    [[SHL:%.*]] = shl <64 x i8> [[EXT]], [[EXT3]]
// CHECK-NEXT:    [[UNPROMOTION:%.*]] = bitcast <64 x i8> [[SHL]] to <64 x b8>
// CHECK-NEXT:    store <64 x b8> [[UNPROMOTION]], ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <vscale x 16 x i8>, ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
fixed_int8_t lshift_i8_rsplat(fixed_int8_t a, int8_t b) {
  return a << b;
}

// CHECK-LABEL: @lshift_i8_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL_COERCE:%.*]] = alloca <vscale x 16 x i8>, align 16
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <vscale x 16 x i8> [[A_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP1:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP0]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <64 x b8> poison, b8 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <64 x b8> [[SPLAT_SPLATINSERT]], <64 x b8> poison, <64 x i32> zeroinitializer
// CHECK-NEXT:    [[EXT:%.*]] = bytecast exact <64 x b8> [[SPLAT_SPLAT]] to <64 x i8>
// CHECK-NEXT:    [[EXT2:%.*]] = bytecast exact <64 x b8> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    [[SHL:%.*]] = shl <64 x i8> [[EXT]], [[EXT2]]
// CHECK-NEXT:    [[UNPROMOTION:%.*]] = bitcast <64 x i8> [[SHL]] to <64 x b8>
// CHECK-NEXT:    store <64 x b8> [[UNPROMOTION]], ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <vscale x 16 x i8>, ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
fixed_int8_t lshift_i8_lsplat(fixed_int8_t a, int8_t b) {
  return b << a;
}

// CHECK-LABEL: @rshift_i8_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL_COERCE:%.*]] = alloca <vscale x 16 x i8>, align 16
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <vscale x 16 x i8> [[A_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP1:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP0]], i64 0)
// CHECK-NEXT:    [[EXT:%.*]] = bytecast exact <64 x b8> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    [[CONV:%.*]] = bytecast exact b8 [[B:%.*]] to i8
// CHECK-NEXT:    [[CONV2:%.*]] = sext i8 [[CONV]] to i32
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <64 x i32> poison, i32 [[CONV2]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <64 x i32> [[SPLAT_SPLATINSERT]], <64 x i32> poison, <64 x i32> zeroinitializer
// CHECK-NEXT:    [[EXT3:%.*]] = trunc <64 x i32> [[SPLAT_SPLAT]] to <64 x i8>
// CHECK-NEXT:    [[SHR:%.*]] = lshr <64 x i8> [[EXT]], [[EXT3]]
// CHECK-NEXT:    [[UNPROMOTION:%.*]] = bitcast <64 x i8> [[SHR]] to <64 x b8>
// CHECK-NEXT:    store <64 x b8> [[UNPROMOTION]], ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <vscale x 16 x i8>, ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
fixed_int8_t rshift_i8_rsplat(fixed_int8_t a, int8_t b) {
  return a >> b;
}

// CHECK-LABEL: @rshift_i8_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL_COERCE:%.*]] = alloca <vscale x 16 x i8>, align 16
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <vscale x 16 x i8> [[A_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP1:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP0]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <64 x b8> poison, b8 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <64 x b8> [[SPLAT_SPLATINSERT]], <64 x b8> poison, <64 x i32> zeroinitializer
// CHECK-NEXT:    [[EXT:%.*]] = bytecast exact <64 x b8> [[SPLAT_SPLAT]] to <64 x i8>
// CHECK-NEXT:    [[EXT2:%.*]] = bytecast exact <64 x b8> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    [[SHR:%.*]] = lshr <64 x i8> [[EXT]], [[EXT2]]
// CHECK-NEXT:    [[UNPROMOTION:%.*]] = bitcast <64 x i8> [[SHR]] to <64 x b8>
// CHECK-NEXT:    store <64 x b8> [[UNPROMOTION]], ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <vscale x 16 x i8>, ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
fixed_int8_t rshift_i8_lsplat(fixed_int8_t a, int8_t b) {
  return b >> a;
}

// CHECK-LABEL: @lshift_u8_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL_COERCE:%.*]] = alloca <vscale x 16 x i8>, align 16
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <vscale x 16 x i8> [[A_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP1:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP0]], i64 0)
// CHECK-NEXT:    [[EXT:%.*]] = bytecast exact <64 x b8> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    [[CONV:%.*]] = bytecast exact b8 [[B:%.*]] to i8
// CHECK-NEXT:    [[CONV2:%.*]] = zext i8 [[CONV]] to i32
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <64 x i32> poison, i32 [[CONV2]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <64 x i32> [[SPLAT_SPLATINSERT]], <64 x i32> poison, <64 x i32> zeroinitializer
// CHECK-NEXT:    [[EXT3:%.*]] = trunc <64 x i32> [[SPLAT_SPLAT]] to <64 x i8>
// CHECK-NEXT:    [[SHL:%.*]] = shl <64 x i8> [[EXT]], [[EXT3]]
// CHECK-NEXT:    [[UNPROMOTION:%.*]] = bitcast <64 x i8> [[SHL]] to <64 x b8>
// CHECK-NEXT:    store <64 x b8> [[UNPROMOTION]], ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <vscale x 16 x i8>, ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
fixed_uint8_t lshift_u8_rsplat(fixed_uint8_t a, uint8_t b) {
  return a << b;
}

// CHECK-LABEL: @lshift_u8_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL_COERCE:%.*]] = alloca <vscale x 16 x i8>, align 16
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <vscale x 16 x i8> [[A_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP1:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP0]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <64 x b8> poison, b8 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <64 x b8> [[SPLAT_SPLATINSERT]], <64 x b8> poison, <64 x i32> zeroinitializer
// CHECK-NEXT:    [[EXT:%.*]] = bytecast exact <64 x b8> [[SPLAT_SPLAT]] to <64 x i8>
// CHECK-NEXT:    [[EXT2:%.*]] = bytecast exact <64 x b8> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    [[SHL:%.*]] = shl <64 x i8> [[EXT]], [[EXT2]]
// CHECK-NEXT:    [[UNPROMOTION:%.*]] = bitcast <64 x i8> [[SHL]] to <64 x b8>
// CHECK-NEXT:    store <64 x b8> [[UNPROMOTION]], ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <vscale x 16 x i8>, ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
fixed_uint8_t lshift_u8_lsplat(fixed_uint8_t a, uint8_t b) {
  return b << a;
}

// CHECK-LABEL: @rshift_u8_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL_COERCE:%.*]] = alloca <vscale x 16 x i8>, align 16
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <vscale x 16 x i8> [[A_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP1:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP0]], i64 0)
// CHECK-NEXT:    [[EXT:%.*]] = bytecast exact <64 x b8> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    [[CONV:%.*]] = bytecast exact b8 [[B:%.*]] to i8
// CHECK-NEXT:    [[CONV2:%.*]] = zext i8 [[CONV]] to i32
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <64 x i32> poison, i32 [[CONV2]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <64 x i32> [[SPLAT_SPLATINSERT]], <64 x i32> poison, <64 x i32> zeroinitializer
// CHECK-NEXT:    [[EXT3:%.*]] = trunc <64 x i32> [[SPLAT_SPLAT]] to <64 x i8>
// CHECK-NEXT:    [[SHR:%.*]] = lshr <64 x i8> [[EXT]], [[EXT3]]
// CHECK-NEXT:    [[UNPROMOTION:%.*]] = bitcast <64 x i8> [[SHR]] to <64 x b8>
// CHECK-NEXT:    store <64 x b8> [[UNPROMOTION]], ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <vscale x 16 x i8>, ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
fixed_uint8_t rshift_u8_rsplat(fixed_uint8_t a, uint8_t b) {
  return a >> b;
}

// CHECK-LABEL: @rshift_u8_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL_COERCE:%.*]] = alloca <vscale x 16 x i8>, align 16
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <vscale x 16 x i8> [[A_COERCE:%.*]] to <vscale x 16 x b8>
// CHECK-NEXT:    [[TMP1:%.*]] = call <64 x b8> @llvm.vector.extract.v64b8.nxv16b8(<vscale x 16 x b8> [[TMP0]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <64 x b8> poison, b8 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <64 x b8> [[SPLAT_SPLATINSERT]], <64 x b8> poison, <64 x i32> zeroinitializer
// CHECK-NEXT:    [[EXT:%.*]] = bytecast exact <64 x b8> [[SPLAT_SPLAT]] to <64 x i8>
// CHECK-NEXT:    [[EXT2:%.*]] = bytecast exact <64 x b8> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    [[SHR:%.*]] = lshr <64 x i8> [[EXT]], [[EXT2]]
// CHECK-NEXT:    [[UNPROMOTION:%.*]] = bitcast <64 x i8> [[SHR]] to <64 x b8>
// CHECK-NEXT:    store <64 x b8> [[UNPROMOTION]], ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <vscale x 16 x i8>, ptr [[RETVAL_COERCE]], align 16
// CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
fixed_uint8_t rshift_u8_lsplat(fixed_uint8_t a, uint8_t b) {
  return b >> a;
}

// CHECK-LABEL: @lshift_i16_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[CONV:%.*]] = sext i16 [[B:%.*]] to i32
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <32 x i32> poison, i32 [[CONV]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <32 x i32> [[SPLAT_SPLATINSERT]], <32 x i32> poison, <32 x i32> zeroinitializer
// CHECK-NEXT:    [[SH_PROM:%.*]] = trunc <32 x i32> [[SPLAT_SPLAT]] to <32 x i16>
// CHECK-NEXT:    [[SHL:%.*]] = shl <32 x i16> [[A]], [[SH_PROM]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.v32i16(<vscale x 8 x i16> poison, <32 x i16> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 8 x i16> [[CAST_SCALABLE]]
//
fixed_int16_t lshift_i16_rsplat(fixed_int16_t a, int16_t b) {
  return a << b;
}

// CHECK-LABEL: @lshift_i16_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <32 x i16> poison, i16 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <32 x i16> [[SPLAT_SPLATINSERT]], <32 x i16> poison, <32 x i32> zeroinitializer
// CHECK-NEXT:    [[SHL:%.*]] = shl <32 x i16> [[SPLAT_SPLAT]], [[A]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.v32i16(<vscale x 8 x i16> poison, <32 x i16> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 8 x i16> [[CAST_SCALABLE]]
//
fixed_int16_t lshift_i16_lsplat(fixed_int16_t a, int16_t b) {
  return b << a;
}

// CHECK-LABEL: @rshift_i16_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[CONV:%.*]] = sext i16 [[B:%.*]] to i32
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <32 x i32> poison, i32 [[CONV]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <32 x i32> [[SPLAT_SPLATINSERT]], <32 x i32> poison, <32 x i32> zeroinitializer
// CHECK-NEXT:    [[SH_PROM:%.*]] = trunc <32 x i32> [[SPLAT_SPLAT]] to <32 x i16>
// CHECK-NEXT:    [[SHR:%.*]] = ashr <32 x i16> [[A]], [[SH_PROM]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.v32i16(<vscale x 8 x i16> poison, <32 x i16> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 8 x i16> [[CAST_SCALABLE]]
//
fixed_int16_t rshift_i16_rsplat(fixed_int16_t a, int16_t b) {
  return a >> b;
}

// CHECK-LABEL: @rshift_i16_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <32 x i16> poison, i16 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <32 x i16> [[SPLAT_SPLATINSERT]], <32 x i16> poison, <32 x i32> zeroinitializer
// CHECK-NEXT:    [[SHR:%.*]] = ashr <32 x i16> [[SPLAT_SPLAT]], [[A]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.v32i16(<vscale x 8 x i16> poison, <32 x i16> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 8 x i16> [[CAST_SCALABLE]]
//
fixed_int16_t rshift_i16_lsplat(fixed_int16_t a, int16_t b) {
  return b >> a;
}

// CHECK-LABEL: @lshift_u16_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[B:%.*]] to i32
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <32 x i32> poison, i32 [[CONV]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <32 x i32> [[SPLAT_SPLATINSERT]], <32 x i32> poison, <32 x i32> zeroinitializer
// CHECK-NEXT:    [[SH_PROM:%.*]] = trunc <32 x i32> [[SPLAT_SPLAT]] to <32 x i16>
// CHECK-NEXT:    [[SHL:%.*]] = shl <32 x i16> [[A]], [[SH_PROM]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.v32i16(<vscale x 8 x i16> poison, <32 x i16> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 8 x i16> [[CAST_SCALABLE]]
//
fixed_uint16_t lshift_u16_rsplat(fixed_uint16_t a, uint16_t b) {
  return a << b;
}

// CHECK-LABEL: @lshift_u16_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <32 x i16> poison, i16 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <32 x i16> [[SPLAT_SPLATINSERT]], <32 x i16> poison, <32 x i32> zeroinitializer
// CHECK-NEXT:    [[SHL:%.*]] = shl <32 x i16> [[SPLAT_SPLAT]], [[A]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.v32i16(<vscale x 8 x i16> poison, <32 x i16> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 8 x i16> [[CAST_SCALABLE]]
//
fixed_uint16_t lshift_u16_lsplat(fixed_uint16_t a, uint16_t b) {
  return b << a;
}

// CHECK-LABEL: @rshift_u16_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[B:%.*]] to i32
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <32 x i32> poison, i32 [[CONV]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <32 x i32> [[SPLAT_SPLATINSERT]], <32 x i32> poison, <32 x i32> zeroinitializer
// CHECK-NEXT:    [[SH_PROM:%.*]] = trunc <32 x i32> [[SPLAT_SPLAT]] to <32 x i16>
// CHECK-NEXT:    [[SHR:%.*]] = lshr <32 x i16> [[A]], [[SH_PROM]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.v32i16(<vscale x 8 x i16> poison, <32 x i16> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 8 x i16> [[CAST_SCALABLE]]
//
fixed_uint16_t rshift_u16_rsplat(fixed_uint16_t a, uint16_t b) {
  return a >> b;
}

// CHECK-LABEL: @rshift_u16_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <32 x i16> @llvm.vector.extract.v32i16.nxv8i16(<vscale x 8 x i16> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <32 x i16> poison, i16 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <32 x i16> [[SPLAT_SPLATINSERT]], <32 x i16> poison, <32 x i32> zeroinitializer
// CHECK-NEXT:    [[SHR:%.*]] = lshr <32 x i16> [[SPLAT_SPLAT]], [[A]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.v32i16(<vscale x 8 x i16> poison, <32 x i16> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 8 x i16> [[CAST_SCALABLE]]
//
fixed_uint16_t rshift_u16_lsplat(fixed_uint16_t a, uint16_t b) {
  return b >> a;
}

// CHECK-LABEL: @lshift_i32_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <16 x i32> poison, i32 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <16 x i32> [[SPLAT_SPLATINSERT]], <16 x i32> poison, <16 x i32> zeroinitializer
// CHECK-NEXT:    [[SHL:%.*]] = shl <16 x i32> [[A]], [[SPLAT_SPLAT]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.v16i32(<vscale x 4 x i32> poison, <16 x i32> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 4 x i32> [[CAST_SCALABLE]]
//
fixed_int32_t lshift_i32_rsplat(fixed_int32_t a, int32_t b) {
  return a << b;
}

// CHECK-LABEL: @lshift_i32_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <16 x i32> poison, i32 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <16 x i32> [[SPLAT_SPLATINSERT]], <16 x i32> poison, <16 x i32> zeroinitializer
// CHECK-NEXT:    [[SHL:%.*]] = shl <16 x i32> [[SPLAT_SPLAT]], [[A]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.v16i32(<vscale x 4 x i32> poison, <16 x i32> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 4 x i32> [[CAST_SCALABLE]]
//
fixed_int32_t lshift_i32_lsplat(fixed_int32_t a, int32_t b) {
  return b << a;
}

// CHECK-LABEL: @rshift_i32_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <16 x i32> poison, i32 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <16 x i32> [[SPLAT_SPLATINSERT]], <16 x i32> poison, <16 x i32> zeroinitializer
// CHECK-NEXT:    [[SHR:%.*]] = ashr <16 x i32> [[A]], [[SPLAT_SPLAT]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.v16i32(<vscale x 4 x i32> poison, <16 x i32> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 4 x i32> [[CAST_SCALABLE]]
//
fixed_int32_t rshift_i32_rsplat(fixed_int32_t a, int32_t b) {
  return a >> b;
}

// CHECK-LABEL: @rshift_i32_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <16 x i32> poison, i32 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <16 x i32> [[SPLAT_SPLATINSERT]], <16 x i32> poison, <16 x i32> zeroinitializer
// CHECK-NEXT:    [[SHR:%.*]] = ashr <16 x i32> [[SPLAT_SPLAT]], [[A]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.v16i32(<vscale x 4 x i32> poison, <16 x i32> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 4 x i32> [[CAST_SCALABLE]]
//
fixed_int32_t rshift_i32_lsplat(fixed_int32_t a, int32_t b) {
  return b >> a;
}

// CHECK-LABEL: @lshift_u32_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <16 x i32> poison, i32 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <16 x i32> [[SPLAT_SPLATINSERT]], <16 x i32> poison, <16 x i32> zeroinitializer
// CHECK-NEXT:    [[SHL:%.*]] = shl <16 x i32> [[A]], [[SPLAT_SPLAT]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.v16i32(<vscale x 4 x i32> poison, <16 x i32> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 4 x i32> [[CAST_SCALABLE]]
//
fixed_uint32_t lshift_u32_rsplat(fixed_uint32_t a, uint32_t b) {
  return a << b;
}

// CHECK-LABEL: @lshift_u32_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <16 x i32> poison, i32 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <16 x i32> [[SPLAT_SPLATINSERT]], <16 x i32> poison, <16 x i32> zeroinitializer
// CHECK-NEXT:    [[SHL:%.*]] = shl <16 x i32> [[SPLAT_SPLAT]], [[A]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.v16i32(<vscale x 4 x i32> poison, <16 x i32> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 4 x i32> [[CAST_SCALABLE]]
//
fixed_uint32_t lshift_u32_lsplat(fixed_uint32_t a, uint32_t b) {
  return b << a;
}

// CHECK-LABEL: @rshift_u32_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <16 x i32> poison, i32 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <16 x i32> [[SPLAT_SPLATINSERT]], <16 x i32> poison, <16 x i32> zeroinitializer
// CHECK-NEXT:    [[SHR:%.*]] = lshr <16 x i32> [[A]], [[SPLAT_SPLAT]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.v16i32(<vscale x 4 x i32> poison, <16 x i32> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 4 x i32> [[CAST_SCALABLE]]
//
fixed_uint32_t rshift_u32_rsplat(fixed_uint32_t a, uint32_t b) {
  return a >> b;
}

// CHECK-LABEL: @rshift_u32_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <16 x i32> @llvm.vector.extract.v16i32.nxv4i32(<vscale x 4 x i32> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <16 x i32> poison, i32 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <16 x i32> [[SPLAT_SPLATINSERT]], <16 x i32> poison, <16 x i32> zeroinitializer
// CHECK-NEXT:    [[SHR:%.*]] = lshr <16 x i32> [[SPLAT_SPLAT]], [[A]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.v16i32(<vscale x 4 x i32> poison, <16 x i32> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 4 x i32> [[CAST_SCALABLE]]
//
fixed_uint32_t rshift_u32_lsplat(fixed_uint32_t a, uint32_t b) {
  return b >> a;
}

// CHECK-LABEL: @lshift_i64_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <8 x i64> poison, i64 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <8 x i64> [[SPLAT_SPLATINSERT]], <8 x i64> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    [[SHL:%.*]] = shl <8 x i64> [[A]], [[SPLAT_SPLAT]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.v8i64(<vscale x 2 x i64> poison, <8 x i64> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 2 x i64> [[CAST_SCALABLE]]
//
fixed_int64_t lshift_i64_rsplat(fixed_int64_t a, int64_t b) {
  return a << b;
}

// CHECK-LABEL: @lshift_i64_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <8 x i64> poison, i64 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <8 x i64> [[SPLAT_SPLATINSERT]], <8 x i64> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    [[SHL:%.*]] = shl <8 x i64> [[SPLAT_SPLAT]], [[A]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.v8i64(<vscale x 2 x i64> poison, <8 x i64> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 2 x i64> [[CAST_SCALABLE]]
//
fixed_int64_t lshift_i64_lsplat(fixed_int64_t a, int64_t b) {
  return b << a;
}

// CHECK-LABEL: @rshift_i64_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <8 x i64> poison, i64 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <8 x i64> [[SPLAT_SPLATINSERT]], <8 x i64> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    [[SHR:%.*]] = ashr <8 x i64> [[A]], [[SPLAT_SPLAT]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.v8i64(<vscale x 2 x i64> poison, <8 x i64> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 2 x i64> [[CAST_SCALABLE]]
//
fixed_int64_t rshift_i64_rsplat(fixed_int64_t a, int64_t b) {
  return a >> b;
}

// CHECK-LABEL: @rshift_i64_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <8 x i64> poison, i64 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <8 x i64> [[SPLAT_SPLATINSERT]], <8 x i64> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    [[SHR:%.*]] = ashr <8 x i64> [[SPLAT_SPLAT]], [[A]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.v8i64(<vscale x 2 x i64> poison, <8 x i64> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 2 x i64> [[CAST_SCALABLE]]
//
fixed_int64_t rshift_i64_lsplat(fixed_int64_t a, int64_t b) {
  return b >> a;
}

// CHECK-LABEL: @lshift_u64_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <8 x i64> poison, i64 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <8 x i64> [[SPLAT_SPLATINSERT]], <8 x i64> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    [[SHL:%.*]] = shl <8 x i64> [[A]], [[SPLAT_SPLAT]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.v8i64(<vscale x 2 x i64> poison, <8 x i64> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 2 x i64> [[CAST_SCALABLE]]
//
fixed_uint64_t lshift_u64_rsplat(fixed_uint64_t a, uint64_t b) {
  return a << b;
}

// CHECK-LABEL: @lshift_u64_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <8 x i64> poison, i64 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <8 x i64> [[SPLAT_SPLATINSERT]], <8 x i64> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    [[SHL:%.*]] = shl <8 x i64> [[SPLAT_SPLAT]], [[A]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.v8i64(<vscale x 2 x i64> poison, <8 x i64> [[SHL]], i64 0)
// CHECK-NEXT:    ret <vscale x 2 x i64> [[CAST_SCALABLE]]
//
fixed_uint64_t lshift_u64_lsplat(fixed_uint64_t a, uint64_t b) {
  return b << a;
}

// CHECK-LABEL: @rshift_u64_rsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <8 x i64> poison, i64 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <8 x i64> [[SPLAT_SPLATINSERT]], <8 x i64> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    [[SHR:%.*]] = lshr <8 x i64> [[A]], [[SPLAT_SPLAT]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.v8i64(<vscale x 2 x i64> poison, <8 x i64> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 2 x i64> [[CAST_SCALABLE]]
//
fixed_uint64_t rshift_u64_rsplat(fixed_uint64_t a, uint64_t b) {
  return a >> b;
}

// CHECK-LABEL: @rshift_u64_lsplat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = call <8 x i64> @llvm.vector.extract.v8i64.nxv2i64(<vscale x 2 x i64> [[A_COERCE:%.*]], i64 0)
// CHECK-NEXT:    [[SPLAT_SPLATINSERT:%.*]] = insertelement <8 x i64> poison, i64 [[B:%.*]], i64 0
// CHECK-NEXT:    [[SPLAT_SPLAT:%.*]] = shufflevector <8 x i64> [[SPLAT_SPLATINSERT]], <8 x i64> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    [[SHR:%.*]] = lshr <8 x i64> [[SPLAT_SPLAT]], [[A]]
// CHECK-NEXT:    [[CAST_SCALABLE:%.*]] = call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.v8i64(<vscale x 2 x i64> poison, <8 x i64> [[SHR]], i64 0)
// CHECK-NEXT:    ret <vscale x 2 x i64> [[CAST_SCALABLE]]
//
fixed_uint64_t rshift_u64_lsplat(fixed_uint64_t a, uint64_t b) {
  return b >> a;
}
