// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// REQUIRES: powerpc-registered-target
// RUN: %clang_cc1 -flax-vector-conversions=none -triple powerpc64-unknown-unknown -emit-llvm %s \
// RUN:   -target-cpu pwr8 -o - | FileCheck %s -check-prefix=BE-PWR8
// RUN: %clang_cc1 -flax-vector-conversions=none -triple powerpc64le-unknown-unknown -emit-llvm %s \
// RUN:   -target-cpu pwr8 -o - | FileCheck %s -check-prefix=LE-PWR8

// RUN: %clang_cc1 -flax-vector-conversions=none -triple powerpc64-unknown-unknown -emit-llvm %s \
// RUN:   -target-cpu pwr9 -o - | FileCheck %s -check-prefix=BE-PWR9
// RUN: %clang_cc1 -flax-vector-conversions=none -triple powerpc64le-unknown-unknown -emit-llvm %s \
// RUN:   -target-cpu pwr9 -o - | FileCheck %s -check-prefix=LE-PWR9
// RUN: %clang_cc1 -flax-vector-conversions=none -triple powerpc-unknown-unknown -emit-llvm %s \
// RUN:   -target-cpu pwr9 -o - | FileCheck %s -check-prefix=BE32-PWR9

#include <altivec.h>
// BE-PWR8-LABEL: @test_ldrmb1(
// BE-PWR8-NEXT:  entry:
// BE-PWR8-NEXT:    [[PTR_ADDR:%.*]] = alloca ptr, align 8
// BE-PWR8-NEXT:    store ptr [[PTR:%.*]], ptr [[PTR_ADDR]], align 8
// BE-PWR8-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[PTR_ADDR]], align 8
// BE-PWR8-NEXT:    [[TMP1:%.*]] = getelementptr i8, ptr [[TMP0]], i32 0
// BE-PWR8-NEXT:    [[LD_LO:%.*]] = call <4 x i32> @llvm.ppc.altivec.lvx(ptr [[TMP0]])
// BE-PWR8-NEXT:    [[LD_HI:%.*]] = call <4 x i32> @llvm.ppc.altivec.lvx(ptr [[TMP1]])
// BE-PWR8-NEXT:    [[MASK1:%.*]] = call <16 x i8> @llvm.ppc.altivec.lvsl(ptr [[TMP0]])
// BE-PWR8-NEXT:    [[SHUFFLE1:%.*]] = call <4 x i32> @llvm.ppc.altivec.vperm(<4 x i32> [[LD_LO]], <4 x i32> [[LD_HI]], <16 x i8> [[MASK1]])
// BE-PWR8-NEXT:    [[SHUFFLE2:%.*]] = call <4 x i32> @llvm.ppc.altivec.vperm(<4 x i32> zeroinitializer, <4 x i32> [[SHUFFLE1]], <16 x i8> <i8 1, i8 2, i8 3, i8 4, i8 5, i8 6, i8 7, i8 8, i8 9, i8 10, i8 11, i8 12, i8 13, i8 14, i8 15, i8 16>)
// BE-PWR8-NEXT:    [[TMP2:%.*]] = bitcast <4 x i32> [[SHUFFLE2]] to <16 x b8>
// BE-PWR8-NEXT:    ret <16 x b8> [[TMP2]]
//
// LE-PWR8-LABEL: @test_ldrmb1(
// LE-PWR8-NEXT:  entry:
// LE-PWR8-NEXT:    [[PTR_ADDR:%.*]] = alloca ptr, align 8
// LE-PWR8-NEXT:    store ptr [[PTR:%.*]], ptr [[PTR_ADDR]], align 8
// LE-PWR8-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[PTR_ADDR]], align 8
// LE-PWR8-NEXT:    [[TMP1:%.*]] = getelementptr i8, ptr [[TMP0]], i32 0
// LE-PWR8-NEXT:    [[LD_LO:%.*]] = call <4 x i32> @llvm.ppc.altivec.lvx(ptr [[TMP0]])
// LE-PWR8-NEXT:    [[LD_HI:%.*]] = call <4 x i32> @llvm.ppc.altivec.lvx(ptr [[TMP1]])
// LE-PWR8-NEXT:    [[MASK1:%.*]] = call <16 x i8> @llvm.ppc.altivec.lvsr(ptr [[TMP0]])
// LE-PWR8-NEXT:    [[SHUFFLE1:%.*]] = call <4 x i32> @llvm.ppc.altivec.vperm(<4 x i32> [[LD_HI]], <4 x i32> [[LD_LO]], <16 x i8> [[MASK1]])
// LE-PWR8-NEXT:    [[TMP2:%.*]] = bitcast <4 x i32> [[SHUFFLE1]] to <16 x b8>
// LE-PWR8-NEXT:    [[TMP3:%.*]] = shufflevector <16 x b8> [[TMP2]], <16 x b8> zeroinitializer, <16 x i32> <i32 0, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30>
// LE-PWR8-NEXT:    ret <16 x b8> [[TMP3]]
//
// BE-PWR9-LABEL: @test_ldrmb1(
// BE-PWR9-NEXT:  entry:
// BE-PWR9-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 8
// BE-PWR9-NEXT:    [[__B_ADDR_I:%.*]] = alloca i64, align 8
// BE-PWR9-NEXT:    [[__RES_I:%.*]] = alloca <16 x b8>, align 16
// BE-PWR9-NEXT:    [[__MASK_I:%.*]] = alloca <16 x b8>, align 16
// BE-PWR9-NEXT:    [[PTR_ADDR:%.*]] = alloca ptr, align 8
// BE-PWR9-NEXT:    store ptr [[PTR:%.*]], ptr [[PTR_ADDR]], align 8
// BE-PWR9-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[PTR_ADDR]], align 8
// BE-PWR9-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 8
// BE-PWR9-NEXT:    store i64 1, ptr [[__B_ADDR_I]], align 8
// BE-PWR9-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 8
// BE-PWR9-NEXT:    [[TMP2:%.*]] = load i64, ptr [[__B_ADDR_I]], align 8
// BE-PWR9-NEXT:    [[SHL_I:%.*]] = shl i64 [[TMP2]], 56
// BE-PWR9-NEXT:    [[TMP3:%.*]] = call <4 x i32> @llvm.ppc.vsx.lxvll(ptr [[TMP1]], i64 [[SHL_I]])
// BE-PWR9-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <16 x b8>
// BE-PWR9-NEXT:    store <16 x b8> [[TMP4]], ptr [[__RES_I]], align 16
// BE-PWR9-NEXT:    [[TMP5:%.*]] = load i64, ptr [[__B_ADDR_I]], align 8
// BE-PWR9-NEXT:    [[SUB_I:%.*]] = sub i64 16, [[TMP5]]
// BE-PWR9-NEXT:    [[CONV_I:%.*]] = trunc i64 [[SUB_I]] to i8
// BE-PWR9-NEXT:    [[TMP6:%.*]] = bitcast i8 [[CONV_I]] to b8
// BE-PWR9-NEXT:    [[TMP7:%.*]] = getelementptr i8, ptr null, i8 [[CONV_I]]
// BE-PWR9-NEXT:    [[TMP8:%.*]] = call <16 x i8> @llvm.ppc.altivec.lvsr(ptr [[TMP7]])
// BE-PWR9-NEXT:    [[TMP9:%.*]] = bitcast <16 x i8> [[TMP8]] to <16 x b8>
// BE-PWR9-NEXT:    store <16 x b8> [[TMP9]], ptr [[__MASK_I]], align 16
// BE-PWR9-NEXT:    [[TMP10:%.*]] = load <16 x b8>, ptr [[__RES_I]], align 16
// BE-PWR9-NEXT:    [[TMP11:%.*]] = bitcast <16 x b8> [[TMP10]] to <4 x b32>
// BE-PWR9-NEXT:    [[TMP12:%.*]] = bytecast <4 x b32> [[TMP11]] to <4 x i32>
// BE-PWR9-NEXT:    [[TMP13:%.*]] = load <16 x b8>, ptr [[__RES_I]], align 16
// BE-PWR9-NEXT:    [[TMP14:%.*]] = bitcast <16 x b8> [[TMP13]] to <4 x b32>
// BE-PWR9-NEXT:    [[TMP15:%.*]] = bytecast <4 x b32> [[TMP14]] to <4 x i32>
// BE-PWR9-NEXT:    [[TMP16:%.*]] = load <16 x b8>, ptr [[__MASK_I]], align 16
// BE-PWR9-NEXT:    [[TMP17:%.*]] = bytecast <16 x b8> [[TMP16]] to <16 x i8>
// BE-PWR9-NEXT:    [[TMP18:%.*]] = call <4 x i32> @llvm.ppc.altivec.vperm(<4 x i32> [[TMP12]], <4 x i32> [[TMP15]], <16 x i8> [[TMP17]])
// BE-PWR9-NEXT:    [[TMP19:%.*]] = bitcast <4 x i32> [[TMP18]] to <16 x b8>
// BE-PWR9-NEXT:    ret <16 x b8> [[TMP19]]
//
// LE-PWR9-LABEL: @test_ldrmb1(
// LE-PWR9-NEXT:  entry:
// LE-PWR9-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 8
// LE-PWR9-NEXT:    [[__B_ADDR_I:%.*]] = alloca i64, align 8
// LE-PWR9-NEXT:    [[__RES_I:%.*]] = alloca <16 x b8>, align 16
// LE-PWR9-NEXT:    [[__MASK_I:%.*]] = alloca <16 x b8>, align 16
// LE-PWR9-NEXT:    [[PTR_ADDR:%.*]] = alloca ptr, align 8
// LE-PWR9-NEXT:    store ptr [[PTR:%.*]], ptr [[PTR_ADDR]], align 8
// LE-PWR9-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[PTR_ADDR]], align 8
// LE-PWR9-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 8
// LE-PWR9-NEXT:    store i64 1, ptr [[__B_ADDR_I]], align 8
// LE-PWR9-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 8
// LE-PWR9-NEXT:    [[TMP2:%.*]] = load i64, ptr [[__B_ADDR_I]], align 8
// LE-PWR9-NEXT:    [[SHL_I:%.*]] = shl i64 [[TMP2]], 56
// LE-PWR9-NEXT:    [[TMP3:%.*]] = call <4 x i32> @llvm.ppc.vsx.lxvll(ptr [[TMP1]], i64 [[SHL_I]])
// LE-PWR9-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <16 x b8>
// LE-PWR9-NEXT:    store <16 x b8> [[TMP4]], ptr [[__RES_I]], align 16
// LE-PWR9-NEXT:    [[TMP5:%.*]] = load i64, ptr [[__B_ADDR_I]], align 8
// LE-PWR9-NEXT:    [[SUB_I:%.*]] = sub i64 16, [[TMP5]]
// LE-PWR9-NEXT:    [[CONV_I:%.*]] = trunc i64 [[SUB_I]] to i8
// LE-PWR9-NEXT:    [[TMP6:%.*]] = bitcast i8 [[CONV_I]] to b8
// LE-PWR9-NEXT:    [[TMP7:%.*]] = getelementptr i8, ptr null, i8 [[CONV_I]]
// LE-PWR9-NEXT:    [[TMP8:%.*]] = call <16 x i8> @llvm.ppc.altivec.lvsr(ptr [[TMP7]])
// LE-PWR9-NEXT:    [[TMP9:%.*]] = bitcast <16 x i8> [[TMP8]] to <16 x b8>
// LE-PWR9-NEXT:    store <16 x b8> [[TMP9]], ptr [[__MASK_I]], align 16
// LE-PWR9-NEXT:    [[TMP10:%.*]] = load <16 x b8>, ptr [[__RES_I]], align 16
// LE-PWR9-NEXT:    [[TMP11:%.*]] = bitcast <16 x b8> [[TMP10]] to <4 x b32>
// LE-PWR9-NEXT:    [[TMP12:%.*]] = bytecast <4 x b32> [[TMP11]] to <4 x i32>
// LE-PWR9-NEXT:    [[TMP13:%.*]] = load <16 x b8>, ptr [[__RES_I]], align 16
// LE-PWR9-NEXT:    [[TMP14:%.*]] = bitcast <16 x b8> [[TMP13]] to <4 x b32>
// LE-PWR9-NEXT:    [[TMP15:%.*]] = bytecast <4 x b32> [[TMP14]] to <4 x i32>
// LE-PWR9-NEXT:    [[TMP16:%.*]] = load <16 x b8>, ptr [[__MASK_I]], align 16
// LE-PWR9-NEXT:    [[TMP17:%.*]] = bytecast <16 x b8> [[TMP16]] to <16 x i8>
// LE-PWR9-NEXT:    [[TMP18:%.*]] = call <4 x i32> @llvm.ppc.altivec.vperm(<4 x i32> [[TMP12]], <4 x i32> [[TMP15]], <16 x i8> [[TMP17]])
// LE-PWR9-NEXT:    [[TMP19:%.*]] = bitcast <4 x i32> [[TMP18]] to <16 x b8>
// LE-PWR9-NEXT:    ret <16 x b8> [[TMP19]]
//
// BE32-PWR9-LABEL: @test_ldrmb1(
// BE32-PWR9-NEXT:  entry:
// BE32-PWR9-NEXT:    [[PTR_ADDR:%.*]] = alloca ptr, align 4
// BE32-PWR9-NEXT:    store ptr [[PTR:%.*]], ptr [[PTR_ADDR]], align 4
// BE32-PWR9-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[PTR_ADDR]], align 4
// BE32-PWR9-NEXT:    [[TMP1:%.*]] = getelementptr i8, ptr [[TMP0]], i32 0
// BE32-PWR9-NEXT:    [[LD_LO:%.*]] = call <4 x i32> @llvm.ppc.altivec.lvx(ptr [[TMP0]])
// BE32-PWR9-NEXT:    [[LD_HI:%.*]] = call <4 x i32> @llvm.ppc.altivec.lvx(ptr [[TMP1]])
// BE32-PWR9-NEXT:    [[MASK1:%.*]] = call <16 x i8> @llvm.ppc.altivec.lvsl(ptr [[TMP0]])
// BE32-PWR9-NEXT:    [[SHUFFLE1:%.*]] = call <4 x i32> @llvm.ppc.altivec.vperm(<4 x i32> [[LD_LO]], <4 x i32> [[LD_HI]], <16 x i8> [[MASK1]])
// BE32-PWR9-NEXT:    [[SHUFFLE2:%.*]] = call <4 x i32> @llvm.ppc.altivec.vperm(<4 x i32> zeroinitializer, <4 x i32> [[SHUFFLE1]], <16 x i8> <i8 1, i8 2, i8 3, i8 4, i8 5, i8 6, i8 7, i8 8, i8 9, i8 10, i8 11, i8 12, i8 13, i8 14, i8 15, i8 16>)
// BE32-PWR9-NEXT:    [[TMP2:%.*]] = bitcast <4 x i32> [[SHUFFLE2]] to <16 x b8>
// BE32-PWR9-NEXT:    ret <16 x b8> [[TMP2]]
//
vector unsigned char test_ldrmb1(char *ptr) { return __vec_ldrmb(ptr, 1); }
